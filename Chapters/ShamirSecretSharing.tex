\chapter{Improved result for Shamir Secret Sharing and m=1}
\label{sec:shamir} 

As in~\cite{EPRINT:BDIR19}, we prove that for every linear $\mdscode{n}{\codedim}{\F_p}$ code $C$, the $(n,t=\codedim)$-secret sharing scheme $Sh_C$ induced by it is leakage resilient for sufficiently large $t$.
We improve from about $t> 0.907\cdot n$ in~\cite{EPRINT:BDIR19} (their full version), to a smaller constant for Shamir and the case of $m=1$ and leakage advantage $\epsilon=2^{-\Omega(n)}$. Our analysis improves the bound for larger constant $m$ as well, but the bound $t=\Omega(n)$ quickly approaches 1 as $m$ grows (similarly to~\cite{EPRINT:BDIR19} analysis, albeit slightly slower).

\begin{theorem}
\label{shamir}
There exists a constant $\epsilon > 0$, so that for every linear $\mdscode{n}{\codedim}{\F_p}$ code $C$, the corresponding Massey $(n,t=\codedim)$-secret sharing scheme $(Sh_C,Rec_C)$ allows for a leakage of a single bit ($m=1$) and error $\leq 2^{-\epsilon n}$, for sufficiently large $n$ and any $t \geq 0.867\cdot n$.
\end{theorem}

Because of space constraints, we shall only present a proof overview for this theorem.
A complete proof is included in Appendix \ref{proof-shamir}.

\noindent{\bfseries Proof overview.} 
We follow a Fourier-analysis based approach. 
We start with a description of~\cite{EPRINT:BDIR19}'s analysis, and explain where our analysis departs from it.
In~\cite{EPRINT:BDIR19} they prove the following upper bound on the leakage advantage achieved by a leakage functions vector $\vec{L}=(L_1,\ldots,L_n)$ for a Massey scheme based on a $\mdscode{n}{\codedim}{\F_p}$ code. 
%is upper bounded (up to a factor of 2) by the following expression, that depends only on the $[n,t,\F_p]$ code $C$. 
Let $H$ denote the generating matrix of $C^\bot$.
Then
\begin{align}\label{eq:start}
\SD{\vec{L}(C)}{\vec{L}(\F^n_p)}&=\sum_{\vec{\ell}\in(\{0,1\}^m)^n}\left|\sum_{\beta\in \F^{\codedim}_p\setminus{\{0\}}}\prod^n_{i=1}{\widehat{\mathbf{1}}}_{\ell_i}(\ip{\beta,h_i})\right|
\end{align}    
%Here $\vec{1}_{\ell_i}$ is an abuse of notation, standing for $\vec{1}_{A_{i,\ell_i}}(x)$ mapping  $\F_p$ to $\{0,1\}$, that outputs 1 if and only if $x\in A_{i,\ell_i}$. Here $A_{i,\ell_i}=\{x\in\F_p|L_i(x)=\ell_i\}$, that is, all inputs on which $L_i$ leaks the value $\ell_i$.

Then, rearrange the sums and take an absolute value of all summands:
\begin{align}\label{eq:intro}
\SD{\mathbf{L}(C)}{\mathbf{L}(\mathbb{F}^n_p)}&\leq \sum_{\beta\in 
\F^{\codedim}_p\setminus{\{0\}}}\prod^n_{i=1}\Big(\sum_{\ell_i}\left|{\widehat{\mathbf{1}}}_{\ell_i}(\ip{\beta,h_i})\right|\Big)\nonumber
\end{align}

Recall that by Observation~\ref{obs:bound}, $2\max_{\mathbf{L}}\SD{\mathbf{L}(C)}{\mathbf{L}(\mathbb{F}^n_p)}$ upper bounds the leakage error of $C$ (but does not necessarily lower-bounds it well).
This step may lose on parameters, but greatly simplifies analysis.\footnote{Indeed, as we demonstrate in Section~\ref{sec:lim}, taking absolute values is very likely suboptimal.} 
To evaluate the above bound, one uses the fact that there are at most $\codedim-1$ zero coordinates $\alpha_i$ in every codeword. Thus, the contribution of some $\beta$ corresponds to
\begin{align}
\Delta_\beta&=\prod_{i\in[n]}\Big(\sum_{\ell_i}\left|\widehat{\vec{1}}_{\ell_i}(\ip{\beta,h_i})\right|\Big)
\end{align}


Since every boolean function's Fourier coefficient $\widehat{f}(0)$ is the largest one (in absolute value), it makes sense to bound the number of such coefficients appearing together.

Then, leaving out many details, they bound the contribution of any individual non-0 coefficients as $\max_{\alpha\neq 0}\widehat{f}(\alpha)$ by some constant $c_1<1$, resulting in a contribution bounded by $c^{\codedim}_1$ of each $\beta$.\footnote{The `max' is enforced by the expression resulting from the Cauchy-Schwartz based expression in the sequel.}
However, bounding every contribution individually turns out to yield very weak bounds on $t=(1-o(1))n$ (even for $m=1$). Cauchy-Schwartz combined with Parseval's identity allows to obtain a better bound. 
Parseval's identity is useful here, as it bounds the $\ell_2$-norm of each $\widehat{\vec{1}}_{\ell_i}$ by $|A_{i,\ell_i}|/p\leq 1$. The final bound takes advantage of the fact that 
$\widehat{f}(0)$ is not much larger than $\widehat{f'}(\alpha)$ for nice boolean functions $f'$.
Then, they replace the $\vec{1}_{\ell_i}$'s involved by such nice functions, in a way that the expression for the bound can only increase, and bound this expression instead. The need for the replacement stems from the fact that nothing is assumed about the locations of the $0$-coefficients in $H\beta^T$ (indeed, these locations vary for different $\beta$'s).

Our improvement here stems by grouping the $\beta$'s by the locations of $H\beta^T$'s 0-coefficients, and bounding the contribution of each group separately. Knowing the locations of 0's allows to use a bound on $\max_{\alpha\neq 0} \widehat{\vec{1}}_{\ell_i}(\alpha)$. %To complete the picture, jumping ahead, our result for random linear MDS codes in Section~\ref{sec:mds}.
